{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "789ddf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langgraph.graph import END, StateGraph, START"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "989082e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google AI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0080a034",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e6aca29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    ")\n",
    "\n",
    "# Define Project schema\n",
    "class Project(BaseModel):\n",
    "    name: str = Field(description=\"Name of the project\")\n",
    "    description: str = Field(description=\"Description of the project\")\n",
    "\n",
    "class Experience(BaseModel):\n",
    "    name: str = Field(description=\"Name of the company\")\n",
    "    role: str = Field(description=\"Role or position held\")\n",
    "    description: str = Field(description=\"Description of the work done/ experience at this company\")\n",
    "\n",
    "# Define Resume schema using BaseModel (Pydantic v2)\n",
    "class Resume(BaseModel):\n",
    "    name: str = Field(description=\"Full name\")\n",
    "    email: str = Field(description=\"Email address\")\n",
    "    phone: str = Field(description=\"Phone number\")\n",
    "    skills: List[str] = Field(description=\"List of technical and soft skills\")\n",
    "    projects: List[Project] = Field(default_factory=list, description=\"List of projects with name and description\")\n",
    "    experience: List[Experience] = Field(default_factory=list, description=\"List of company experiences with name, role, and description\")\n",
    "    major_name: str = Field(description=\"Major or field of study\")\n",
    "    usc_college_name: str = Field(description=\"Name of the college or university\")\n",
    "    linkedin: str = Field(default=\"\", description=\"LinkedIn profile URL\")\n",
    "\n",
    "# Define State using TypedDict\n",
    "class ResumeParserState(TypedDict):\n",
    "    resume_text: str\n",
    "    parsed_resume: Resume\n",
    "\n",
    "# Define extraction instructions\n",
    "extraction_prompt = \"\"\"You are an expert resume parser. \n",
    "Extract ALL the following information from the resume and return it in the specified JSON format.\n",
    "\n",
    "Resume:\n",
    "{resume_text}\n",
    "\n",
    "IMPORTANT: \n",
    "- For projects: Extract EVERY project mentioned. Create a list where each project has a \"name\" and \"description\".\n",
    "- Include all details about what was done in each project.\n",
    "- Do not skip any projects.\n",
    "- STICK TO WHAT IS MENTIONED IN THE RESUME. DO NOT MAKE UP ANY INFORMATION.\n",
    "\n",
    "Extract:\n",
    "- name: Full name\n",
    "- email: Email address\n",
    "- phone: Phone number\n",
    "- skills: List of technical and soft skills (as a list)\n",
    "- projects: List of projects. EACH project should have \"name\" (project title) and \"description\" (detailed description of what was done)\n",
    "- experience: List of professional or research experiences. EACH experience should have a \"name of the comapny\", \"role at work\" and brief \"description\"\n",
    "- major_name: Major or field of study\n",
    "- usc_college_name: Name of the college or university\n",
    "- linkedin: LinkedIn profile URL (if available, otherwise empty string)\n",
    "\n",
    "Return valid JSON matching the Resume schema.\"\"\"\n",
    "\n",
    "# Define nodes\n",
    "def parse_resume_node(state: ResumeParserState):\n",
    "    \"\"\"Node to parse resume using LLM with structured output\"\"\"\n",
    "    \n",
    "    resume_text = state[\"resume_text\"]\n",
    "    \n",
    "    # Use structured output to enforce Resume format\n",
    "    structured_llm = llm.with_structured_output(Resume, method=\"json_mode\")\n",
    "    \n",
    "    # Create prompt\n",
    "    system_message = extraction_prompt.format(resume_text=resume_text)\n",
    "    \n",
    "    # Invoke LLM\n",
    "    parsed_resume = structured_llm.invoke([\n",
    "        SystemMessage(content=system_message),\n",
    "        HumanMessage(content=\"Please parse this resume and extract all information. Make sure to extract all projects with their descriptions!\")\n",
    "    ])\n",
    "    \n",
    "    return {\"parsed_resume\": parsed_resume}\n",
    "\n",
    "# Build graph\n",
    "def create_resume_parser_graph():\n",
    "    \"\"\"Create and compile the resume parser graph\"\"\"\n",
    "    \n",
    "    builder = StateGraph(ResumeParserState)\n",
    "    builder.add_node(\"parse_resume\", parse_resume_node)\n",
    "    \n",
    "    builder.add_edge(START, \"parse_resume\")\n",
    "    builder.add_edge(\"parse_resume\", END)\n",
    "    \n",
    "    return builder.compile()\n",
    "\n",
    "# Main function\n",
    "def parse_resume(resume_text: str) -> Resume:\n",
    "    \"\"\"Main function to parse a resume\"\"\"\n",
    "    \n",
    "    graph = create_resume_parser_graph()\n",
    "    \n",
    "    result = graph.invoke({\n",
    "        \"resume_text\": resume_text,\n",
    "    })\n",
    "    \n",
    "    return result[\"parsed_resume\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41ff632",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\C'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\C'\n",
      "C:\\Users\\Harsh Toshniwal\\AppData\\Local\\Temp\\ipykernel_25160\\3409489616.py:3: SyntaxWarning: invalid escape sequence '\\C'\n",
      "  sample_resume = \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='John Doe' email='john.doe@email.com' phone='(555) 123-4567' skills=['Python', 'JavaScript', 'LangGraph', 'FastAPI', 'SQL'] projects=[Project(name='Multi-Modal Summarization of MRI Brain Reports and Images', description='- Created a multi-modal deep learning pipeline to generate summaries of MRI brain scans and corresponding radiology reports.\\n- Processed MRI images using a CNN-based feature extractor (ResNet50) and combined image embeddings with text embeddings'), Project(name='LegalAI', description='- Developed a conversational law firm recommendation system across 45+ firms integrating the clustering and ML models for predictions.\\n- The agent handles case registration, provide predictive insights with interactive Q&A, leveraging ML models with 79 percent accuracy. Implemented Human-in-the-loop to add other matter details before making new predictions and storing them in the DB.\\n- Architected full-stack microservices platform with Node.js/Express backend, React/WebSocket frontend and MySQL database')] experience=[Experience(name='Tech Corp', role='Senior Developer', description='- Led development of AI applications\\n- Managed team of 5 engineers'), Experience(name='IAMSEC VISION IT Pvt. Ltd.', role='Data Scientist', description='● Deployed a CI\\\\CD agentic pipeline for social media analytics using Metas Graph API,LangGraph for engagement, competitor analysis.\\n● Defined a pipeline to extract data from unstructured documents using metadata enrichment, hierarchical chunking and graph relations. Subsequently stored the data in Neo4j and designed a GraphRAG, it improved the retrieval quality by 40 percent compared to regular RAG.\\n● Migrated 3 clients MySQL database on AWS RDS, reducing downtime by average of 35 percent and improving query performance by 30%.'), Experience(name='Wolters Kluwer Pvt Ltd India.', role='Data Science Intern', description='● Played a key role in building a chatbot using LangChain. Designed evaluation framework using Mlflow-based logging system to capture LLM CoT, metrics, and user feedback,hallucination detection. Researched on advanced RAG techniques like-HyDe, Query decomposition, trained embedding models to improve the search and retrieval workflow.\\n● Streamlined client-specific ETL pipelines for data aggregation from multiple file formats, and devised usable 10k records. After hypothetical testing designed a custom median encoding strategy to train the ML models to predict multiple labels. Used the prediction to develope a law firm recommendation system tailored to client-specific needs, improving baseline accuracy to ~72% (an 8 percent gain).\\n● Optimized prompts for RAG, agent handling SQL DB by trying methods like: 1. ReAct, 2. Chain-Of-Thoughts, 3. Multimodal CoT.'), Experience(name='Dr. Devika Verma Pune', role='Natural Language Processing Research Intern', description='● Implemented kāraka extraction methods: 1. data-driven classifier and 2. Universal Dependency parser with UD to Kāraka mappings.\\n● Engineered feature vectors for (question, candidate sentence) pairs based on verb alignment, kāraka arguments, and post-positions.\\n● Achieved 82.7 percent accuracy in Hindi (data-driven annotator) and 68.7 percent in Marathi, with MRR of 0.71 and 0.64, respectively')] major_name='Computer Science' usc_college_name='Viterbi School of Engineering, University of Southern California' linkedin=''\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    sample_resume = \"\"\"\n",
    "    John Doe\n",
    "    john.doe@email.com\n",
    "    (555) 123-4567\n",
    "    \n",
    "    SKILLS\n",
    "    Python, JavaScript, LangGraph, FastAPI, SQL\n",
    "    \n",
    "    EXPERIENCE\n",
    "    Senior Developer at Tech Corp (2021-2023)\n",
    "    - Led development of AI applications\n",
    "    - Managed team of 5 engineers\n",
    "    \n",
    "    EDUCATION\n",
    "    B.S. Computer Science\n",
    "    Viterbi School of Engineering, University of Southern California\n",
    "\n",
    "    EXPERIENCE\n",
    "    IAMSEC VISION IT Pvt. Ltd. \n",
    "    Data Scientist \n",
    "    ● Deployed a CI\\CD agentic pipeline for social media analytics using Metas Graph API,LangGraph for engagement, competitor analysis.\n",
    "    ● Defined a pipeline to extract data from unstructured documents using metadata enrichment, hierarchical chunking and graph relations. Subsequently stored the data in Neo4j and designed a GraphRAG, it improved the retrieval quality by 40 percent compared to regular RAG.\n",
    "    ● Migrated 3 clients MySQL database on AWS RDS, reducing downtime by average of 35 percent and improving query performance by 30%.\n",
    "    Wolters Kluwer Pvt Ltd India.\n",
    "    Data Science Intern\n",
    "    ● Played a key role in building a chatbot using LangChain. Designed evaluation framework using Mlflow-based logging system to capture LLM CoT, metrics, and user feedback,hallucination detection. Researched on advanced RAG techniques like-HyDe, Query decomposition, trained embedding models to improve the search and retrieval workflow.\n",
    "    ● Streamlined client-specific ETL pipelines for data aggregation from multiple file formats, and devised usable 10k records. After hypothetical testing designed a custom median encoding strategy to train the ML models to predict multiple labels. Used the prediction to develope a law firm recommendation system tailored to client-specific needs, improving baseline accuracy to ~72% (an 8 percent gain).\n",
    "    ● Optimized prompts for RAG, agent handling SQL DB by trying methods like: 1. ReAct, 2. Chain-Of-Thoughts, 3. Multimodal CoT.\n",
    "    Dr. Devika Verma\n",
    "    Natural Language Processing Research Intern \n",
    "    ● Implemented kāraka extraction methods: 1. data-driven classifier and 2. Universal Dependency parser with UD to Kāraka mappings.\n",
    "    ● Engineered feature vectors for (question, candidate sentence) pairs based on verb alignment, kāraka arguments, and post-positions.\n",
    "    ● Achieved 82.7 percent accuracy in Hindi (data-driven annotator) and 68.7 percent in Marathi, with MRR of 0.71 and 0.64, respectively\n",
    "\n",
    "    PROJECTS\n",
    "    1. Multi-Modal Summarization of MRI Brain Reports and Images\n",
    "    - Created a multi-modal deep learning pipeline to generate summaries of MRI brain scans and corresponding radiology reports.\n",
    "    - Processed MRI images using a CNN-based feature extractor (ResNet50) and combined image embeddings with text embeddings\n",
    "\n",
    "    2. LegalAI\n",
    "    - Developed a conversational law firm recommendation system across 45+ firms integrating the clustering and ML models for predictions.\n",
    "    - The agent handles case registration, provide predictive insights with interactive Q&A, leveraging ML models with 79 percent accuracy. Implemented Human-in-the-loop to add other matter details before making new predictions and storing them in the DB.\n",
    "    - Architected full-stack microservices platform with Node.js/Express backend, React/WebSocket frontend and MySQL database\n",
    "    \"\"\"\n",
    "    \n",
    "    parsed = parse_resume(sample_resume)\n",
    "    print(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06b650d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lc_aca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
